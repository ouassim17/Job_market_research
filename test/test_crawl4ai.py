import asyncio
import os
from crawl4ai import AsyncWebCrawler
from openai import OpenAI

# --- Configuration ---
OPENAI_API_KEY = "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"  # Remplace par ta clÃ© OpenAI
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
client = OpenAI(api_key=OPENAI_API_KEY)

# URL cible
URL = "https://www.rekrute.com/offres.html?st=d&keywordNew=1&jobLocation=RK&tagSearchKey=&keyword=data"

# Prompt pour GPT
PROMPT = """
Tu es un extracteur spÃ©cialisÃ© en offres d'emploi.

Ã€ partir de cette page Rekrute.com, extrait toutes les offres disponibles sous forme de tableau JSON contenant :
- Titre de l'offre
- Nom de l'entreprise
- URL de l'offre
- Date de publication
- Type de contrat
- RÃ©gion
- Secteur d'activitÃ©
- Niveau d'expÃ©rience requis
- Niveau d'Ã©tudes requis
- CompÃ©tences clÃ©s

Format de sortie : uniquement un tableau JSON valide, sans texte supplÃ©mentaire.
"""

async def scrape_and_extract_with_gpt():
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url=URL,
            js_code="""
            // Attendre que les offres soient chargÃ©es
            const waitForJobs = () => {
                if (document.querySelectorAll('.offer').length > 0) {
                    return 'Offres trouvÃ©es';
                }
                return false;
            };
            return waitForJobs();
            """
        )

        if not result.success:
            print("[âŒ] Ã‰chec lors du chargement de la page.")
            return None

        print("[âœ…] Page chargÃ©e avec succÃ¨s.")

        # Appel Ã  l'API OpenAI pour extraire les donnÃ©es
        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": "Vous Ãªtes un assistant spÃ©cialisÃ© dans l'extraction structurÃ©e d'informations."},
                {"role": "user", "content": f"{PROMPT}\n\nVoici le HTML de la page emploi :\n\n{result.cleaned_html}"}
            ]
        )

        extracted_data = completion.choices[0].message.content
        return extracted_data


async def main():
    print("[ğŸ”] DÃ©marrage du scraping et extraction IA...\n")
    extracted_content = await scrape_and_extract_with_gpt()

    if extracted_content:
        print("\nğŸ§  DonnÃ©es extraites par GPT :")
        print(extracted_content)

        # Sauvegarder dans un fichier JSON
        with open("offres_emploi_gpt.json", "w", encoding="utf-8") as f:
            f.write(extracted_content)

        print("\nğŸ’¾ DonnÃ©es sauvegardÃ©es dans 'offres_emploi_gpt.json'")
    else:
        print("\nâŒ Aucune donnÃ©e extraite.")


if __name__ == "__main__":
    asyncio.run(main())